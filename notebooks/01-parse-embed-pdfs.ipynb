{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PDFPlumberLoader,\n",
    "    AzureAIDocumentIntelligenceLoader,\n",
    ")\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "os.environ[\"UPSTAGE_API_KEY\"] = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Peraturan_Kemahasiswaan_2022.pdf\"\n",
    "# filename = \"Peraturan_Akademik_2021.pdf\"\n",
    "\n",
    "loader = AzureAIDocumentIntelligenceLoader(\n",
    "    api_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_API_KEY\"),\n",
    "    file_path=\"pdf/\" + filename,\n",
    "    api_model=\"prebuilt-read\",\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "\n",
    "pickle.dump(data, open(\"pickle_pdf/\" + filename + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Peraturan_Akademik_2021.pdf\"\n",
    "data = pickle.load(open(\"pickle_pdf/\" + filename + \".pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of chapters: 15\n",
      "Num of sections in Chapter 0 : 0\n",
      "Num of sections in Chapter 1 : 1\n",
      "Num of sections in Chapter 2 : 10\n",
      "Num of sections in Chapter 3 : 8\n",
      "Num of sections in Chapter 4 : 7\n",
      "Num of sections in Chapter 5 : 9\n",
      "Num of sections in Chapter 6 : 16\n",
      "Num of sections in Chapter 7 : 12\n",
      "Num of sections in Chapter 8 : 2\n",
      "Num of sections in Chapter 9 : 1\n",
      "Num of sections in Chapter 10 : 1\n",
      "Num of sections in Chapter 11 : 1\n",
      "Num of sections in Chapter 12 : 1\n",
      "Num of sections in Chapter 13 : 1\n",
      "Num of sections in Chapter 14 : 1\n",
      "Total sections: 71\n"
     ]
    }
   ],
   "source": [
    "def parse_documents(data: Document) -> list[Document]:\n",
    "    all_text = data.page_content\n",
    "\n",
    "    # split the all_text based on the occurence of the \"BAB <roman number>\" string. use regex\n",
    "    chapters = re.split(r\"BAB\\s*[IVXLCDM]+\\s*\", all_text)\n",
    "    # chapters = all_text.split(\"BAB \")\n",
    "    chapters = [\"BAB \" + text for text in chapters]\n",
    "    print(\"Num of chapters:\", len(chapters))\n",
    "\n",
    "    # front page\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=chapters[0], metadata={\"judul\": filename, \"bab\": 0, \"pasal\": 0}\n",
    "        )\n",
    "    ]\n",
    "    total_sections = 0\n",
    "\n",
    "    # print(chapters[5])\n",
    "    for i, chapter in enumerate(chapters):\n",
    "        # split each chapter based on the occurence of \"Pasal <INTEGER>\" format followed with a \\n\n",
    "        # the \\n may not be directly after the \"Pasal <INTEGER>\". use regex\n",
    "        # each section is a pasal\n",
    "\n",
    "        # sections = re.split(r\"Pasal\\s+\\d+\\s+\\n\", chapter)\n",
    "        sections = re.split(r\"\\nPasal \\d+\", chapter)\n",
    "        sections = [s for s in sections if \"BAB\" not in s]\n",
    "\n",
    "        print(\"Num of sections in Chapter\", i, \":\", len(sections))\n",
    "\n",
    "        documents += [\n",
    "            Document(\n",
    "                page_content=section,\n",
    "                metadata={\"judul\": filename, \"bab\": i, \"pasal\": total_sections + j + 1},\n",
    "            )\n",
    "            for j, section in enumerate(sections)\n",
    "        ]\n",
    "\n",
    "        total_sections += len(sections)\n",
    "\n",
    "    print(\"Total sections:\", total_sections)\n",
    "\n",
    "    # remove whitespaces from each page content (e.g. \\n)\n",
    "    # page content is stored in the page_content property of the document object\n",
    "    # also remove double spaces\n",
    "    for doc in documents:\n",
    "        doc.page_content = \" \".join(doc.page_content.split())\n",
    "        doc.page_content = doc.page_content.replace(\"  \", \" \")\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "res = parse_documents(data[0])\n",
    "pickle.dump(res, open(\"pickle_res/\" + filename + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newo/Documents/itb-document-chatbot/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/newo/Documents/itb-document-chatbot/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "Some weights of the model checkpoint at infgrad/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# read the pickle_res folder, load all the pickle files, and concatenate the page_content of each document\n",
    "\n",
    "res_files = os.listdir(\"pickle_res\")\n",
    "res = []\n",
    "\n",
    "for file in res_files:\n",
    "    res += pickle.load(open(\"pickle_res/\" + file, \"rb\"))\n",
    "\n",
    "# embed results\n",
    "model = SentenceTransformer(\"infgrad/stella_en_400M_v5\", trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "input_texts = [doc.page_content for doc in res]\n",
    "embeddings = model.encode(input_texts, normalize_embeddings=False)\n",
    "\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_588814/1907614111.py:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(os.getenv(\"QDRANT_URL\"))\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"peraturan_stella\",\n",
    "    vectors_config=VectorParams(size=len(embeddings[0]), distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=\"peraturan_stella\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=vector,\n",
    "            payload={\n",
    "                \"page_content\": res[i].page_content,\n",
    "                \"metadata\": res[i].metadata,\n",
    "            },\n",
    "        )\n",
    "        for i, vector in enumerate(embeddings)\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import Libraries, Initialize Models, etc\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 66,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_core.documents import Document\n",
                "from langchain_openai import OpenAIEmbeddings\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "from qdrant_client import QdrantClient\n",
                "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
                "\n",
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "load_dotenv(\"../.env\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model_name = \"BAAI/bge-m3\"\n",
                "\n",
                "# embedding_model = HuggingFaceInferenceAPIEmbeddings(\n",
                "#     api_key=os.getenv(\"HUGGINGFACE_API_KEY\"), model_name=model_name\n",
                "# )\n",
                "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
                "\n",
                "# # test if embedding model works\n",
                "# res = embedding_model.embed_query(\"The quick brown fox jumps over the lazy dog\")\n",
                "# res[:3]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Sheets\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [],
            "source": [
                "def read_dfs(sheet_names: list[str]) -> dict[str, pd.DataFrame]:\n",
                "    # filepath = \"xlsx/[ANNOTATED] Informasi Umum ITB.xlsx\"\n",
                "    # dfs = pd.read_excel(filepath, sheet_name=sheet_names)\n",
                "    url = \"https://docs.google.com/spreadsheets/d/1p0_lwcGKLP5NtsLx_cd5kCyc7i58VuTV/edit?usp=sharing&ouid=103581786644820929582&rtpof=true&sd=true\"\n",
                "    url_pandas = url.replace(\"/edit?usp=sharing\", \"/export?format=xlsx\")\n",
                "    dfs = pd.read_excel(url_pandas, sheet_name=sheet_names)\n",
                "\n",
                "    for name in sheet_names:\n",
                "        # for each df, drop a row if the entire row is null\n",
                "        # also drop cells with the content \"kembali ke halaman utama\"\n",
                "        dfs[name] = dfs[name].dropna(how=\"all\").reset_index(drop=True)\n",
                "        dfs[name] = dfs[name][\n",
                "            ~dfs[name].iloc[:, 0].str.contains(\"Kembali ke Halaman Utama\", na=False)\n",
                "        ]\n",
                "\n",
                "        # # do not make the first row as the title of the column\n",
                "        # dfs[name].columns = range(dfs[name].shape[1])\n",
                "\n",
                "    return dfs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process Sheets\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHUNK_SIZE = 1000\n",
                "CHUNK_OVERLAP = 0\n",
                "\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=CHUNK_SIZE,\n",
                "    chunk_overlap=CHUNK_OVERLAP,\n",
                "    length_function=len,\n",
                "    is_separator_regex=False,\n",
                ")\n",
                "\n",
                "\n",
                "def process_standard_sheets(sheet_name: str, df: pd.DataFrame):\n",
                "    # we want to read cell by cell: there is only one column in this dataframe\n",
                "    # essentially, we want to group the content between cells with the tag [H1]\n",
                "    # get the indices of the cells with the tag [H1]\n",
                "    documents = []\n",
                "    h1_indices = df[df.iloc[:, 0].str.contains(\"\\[H1\\]\")].index\n",
                "    ref = df.columns[0].split(\"Referensi: \", 1)[1]\n",
                "\n",
                "    for i in range(len(h1_indices)):\n",
                "        index = h1_indices[i]\n",
                "        next_idx = h1_indices[i + 1] if i + 1 < len(h1_indices) else len(df)\n",
                "\n",
                "        # get the title and the content of the part between the [H1] tags\n",
                "        title = df.iloc[index, 0]\n",
                "        content = \"\".join(str(df.iloc[index + 1 : next_idx, 0].values))\n",
                "        content = content.replace(\"\\\\xa0\", \" \").replace(\"\\n\", \"\")\n",
                "\n",
                "        title = title.replace(\"[H1] \", \"\")\n",
                "\n",
                "        texts = text_splitter.create_documents([content])\n",
                "\n",
                "        for i in range(len(texts)):\n",
                "            texts[i].metadata = {\n",
                "                \"title\": title,\n",
                "                \"sheet_name\": sheet_name,\n",
                "                \"reference\": ref,\n",
                "            }\n",
                "            texts[i].page_content = f\"{texts[i].page_content} - Reference: {ref}\"\n",
                "\n",
                "        documents.extend(texts)\n",
                "\n",
                "    return documents\n",
                "\n",
                "\n",
                "def process_sheets_with_tables(sheet_name: str, df: pd.DataFrame):\n",
                "    documents = []\n",
                "    h1_indices = df[df.iloc[:, 0].str.contains(\"\\[H1\\]\", na=False)].index\n",
                "    table_indices = df[df.iloc[:, 0].str.contains(\"\\[TABLE\\]\", na=False)].index\n",
                "    ref = df.columns[0].split(\"Referensi: \", 1)[1]\n",
                "\n",
                "    for i in range(len(h1_indices)):\n",
                "        index = h1_indices[i]\n",
                "        next_idx = h1_indices[i + 1] if i + 1 < len(h1_indices) else len(df)\n",
                "\n",
                "        # find the table index that is more than index but less than next idx\n",
                "        table_index = table_indices[\n",
                "            (table_indices > index) & (table_indices < next_idx)\n",
                "        ]\n",
                "\n",
                "        # update the next_idx to be the table index if it exists\n",
                "        if len(table_index) > 0:\n",
                "            next_idx = table_index[0]\n",
                "\n",
                "        # get the title and the content of the part between the [H1] tags\n",
                "        title = df.iloc[index, 0]\n",
                "        content = \"\".join(str(df.iloc[index + 1 : next_idx, 0].values))\n",
                "        content = content.replace(\"\\\\xa0\", \" \").replace(\"\\n\", \"\")\n",
                "\n",
                "        title = title.replace(\"[H1] \", \"\")\n",
                "\n",
                "        texts = text_splitter.create_documents([content])\n",
                "\n",
                "        for i in range(len(texts)):\n",
                "            texts[i].metadata = {\"title\": title, \"sheet_name\": sheet_name}\n",
                "            texts[i].page_content = f\"{texts[i].page_content} - Reference: {ref}\"\n",
                "\n",
                "        documents.extend(texts)\n",
                "\n",
                "    return documents"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process Tables\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
                "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
                "\n",
                "\n",
                "def read_tables():\n",
                "    url = \"https://docs.google.com/spreadsheets/d/15nTfetJP_EO4uN7ATwVTPti12v_DppC4O49zbxgM1PE/edit?usp=sharing\"\n",
                "    url_pandas = url.replace(\"/edit?usp=sharing\", \"/export?format=xlsx\")\n",
                "    xls = pd.ExcelFile(url_pandas)\n",
                "\n",
                "    dfs = {}\n",
                "    for sheet_name in xls.sheet_names:\n",
                "        df = xls.parse(sheet_name)\n",
                "        dfs[sheet_name] = df\n",
                "\n",
                "    print(\"total dfs:\", len(dfs))\n",
                "    return dfs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from groq import Groq\n",
                "\n",
                "\n",
                "PARSE_TABLE_SYSTEM_PROMPT = \"\"\"\n",
                "You are an agent specializing in converting a list of JSON representations into paragraphs of text in order to make it readable.\n",
                "Avoid converting it into a list; prefer converting the representation into a readable sentence or paragraph.\n",
                "You may segment the paragraphs to what you see fit. Only respond with the representation. \n",
                "\"\"\"\n",
                "PARSE_TABLE_USER_PROMPT = \"\"\"\n",
                "Please convert this JSON representation into paragraphs of text:\n",
                "{jsons}\n",
                "\"\"\"\n",
                "\n",
                "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
                "\n",
                "\n",
                "def parse_tables(df: pd.DataFrame, sheet_name: str):\n",
                "    ref = df.columns[0].split(\"Referensi: \", 1)[1]\n",
                "    jsons = []\n",
                "    docs = []\n",
                "\n",
                "    # replace the header with the first row\n",
                "    df.columns = df.iloc[0]\n",
                "    df = df[1:]\n",
                "\n",
                "    # convert this into a list of json, where each key is the column and each value is the value\n",
                "    for i in range(len(df)):\n",
                "        row = df.iloc[i]\n",
                "        text = row.to_json()\n",
                "        text = text.replace(\"\\\\xa0\", \" \").replace(\"\\n\", \"\")\n",
                "        jsons.append(text)\n",
                "\n",
                "    user_prompt = PARSE_TABLE_USER_PROMPT.format(jsons=jsons)\n",
                "    completion = client.chat.completions.create(\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": PARSE_TABLE_SYSTEM_PROMPT},\n",
                "            {\"role\": \"user\", \"content\": user_prompt},\n",
                "        ],\n",
                "        temperature=0,\n",
                "        model=\"llama-3.1-8b-instant\",\n",
                "    )\n",
                "\n",
                "    content = completion.choices[0].message.content\n",
                "\n",
                "    texts = text_splitter.create_documents([content])\n",
                "\n",
                "    for i in range(len(texts)):\n",
                "        texts[i].metadata = {\"title\": sheet_name, \"sheet_name\": sheet_name}\n",
                "        texts[i].page_content = f\"{texts[i].page_content} - Reference: {ref}\"\n",
                "\n",
                "    docs.extend(texts)\n",
                "\n",
                "    return docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "total dfs: 23\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "507"
                        ]
                    },
                    "execution_count": 85,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "all_documents = []\n",
                "\n",
                "\n",
                "# standard sheets: readable ones with one column\n",
                "standard_sheets = [\n",
                "    \"Tentang ITB\",\n",
                "    \"Penerimaan\",\n",
                "    \"Pendidikan\",\n",
                "    \"Penelitian\",\n",
                "    \"Pengabdian\",\n",
                "    \"Multikampus\",\n",
                "    \"FAQ Umum\",\n",
                "    \"FAQ Beasiswa\",\n",
                "    \"FAQ Penerimaan Mahasiswa Baru\",\n",
                "    \"FAQ Teknik Fisika\",\n",
                "    \"FAQ Lain-lain\",\n",
                "]\n",
                "\n",
                "sheets_with_tables = [\n",
                "    \"Info Pendaftaran - S1 - Cleaned\",\n",
                "    \"Info Pendaftaran - S2\",\n",
                "    \"Info Pendaftaran - S3\",\n",
                "    \"International Program - S1\",\n",
                "    \"International Program - S2\",\n",
                "    \"International Program - S3\",\n",
                "    \"International Program - Student\",\n",
                "    \"International Program - Scholar\",\n",
                "    \"Program Profesi - Keinsinyuran\",\n",
                "    \"Program Profesi - Apoteker\",\n",
                "    \"Program Nonreguler - NRNG\",\n",
                "    \"Program Nonreguler - MBKM ITB X\",\n",
                "    \"Program Nonreguler - Summer Cou\",\n",
                "]\n",
                "\n",
                "\n",
                "# read standard sheets\n",
                "standard_dfs = read_dfs(standard_sheets)\n",
                "sheets_with_tables_dfs = read_dfs(sheets_with_tables)\n",
                "table_dfs = read_tables()\n",
                "\n",
                "for sheet_name, df in standard_dfs.items():\n",
                "    docs = process_standard_sheets(sheet_name, df)\n",
                "    all_documents.extend(docs)\n",
                "\n",
                "for sheet_name, df in sheets_with_tables_dfs.items():\n",
                "    docs = process_sheets_with_tables(sheet_name, df)\n",
                "    all_documents.extend(docs)\n",
                "\n",
                "for sheet_name, df in table_dfs.items():\n",
                "    docs = parse_tables(df, sheet_name)\n",
                "    all_documents.extend(docs)\n",
                "\n",
                "\n",
                "len(all_documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[Document(metadata={'title': 'ITB IUP Program Sched', 'sheet_name': 'ITB IUP Program Sched'}, page_content='The ITB International Undergraduate Program has several key dates to note. Online registration for the program will take place, with the first period scheduled from 13 February to 28 March 2024 and the second period from 30 April to 23 June 2024.\\n\\nThe ITB ELQ/AQ Test Simulation will be conducted on two separate dates. The first test will take place on 1 April 2024 from 13.00-14.00 WIB (UTC+7), while the second test will be held on 24 June 2024 from 13.00-14.15 WIB (UTC+7).\\n\\nThe ITB English Language Qualification Test (ITB ELQ Test) will be administered on two different dates. The first test will take place on 2 April 2024 from 13.00-15.00 WIB (UTC+7), while the second test will be held on 27 June 2024 from 13.00-17.00 WIB (UTC+7).\\n\\nThe ITB Academic Qualification Test (ITB AQ Test) will also be conducted on two separate dates. The first test will take place on 4 April 2024 from 13.00-16.30 WIB (UTC+7), while the second test will be held on 29 June 2024 from 13.00-16.45 WIB (UTC+7). - Reference: https://admission.itb.ac.id/info/international-undergraduate-program/'),\n",
                            " Document(metadata={'title': 'ITB IUP Program Sched', 'sheet_name': 'ITB IUP Program Sched'}, page_content='Drawing tests will be administered for students applying to the Architecture Study Program and the Faculty of Fine Arts and Design. For the Architecture Study Program, the first test will take place on 3 April 2024 from 13.00-14.30 WIB (UTC+7), while the second test will be held on 26 June 2024 from 13.00-14.30 WIB. For the Faculty of Fine Arts and Design, the first test will take place on 3 April 2024 from 15.00-17.30 WIB (UTC+7), while the second test will be held on 26 June 2024 from 15.00-17.30 WIB (UTC+7).\\n\\nThe selection result announcement will be made on two separate dates. The first announcement will be made on 17 April 2024, while the second announcement will be made on 1 July 2024.\\n\\nNew student registration for the ITB International Undergraduate Program will take place in two separate periods. The first period will be from 29 April to 3 May 2024, while the second period will be from 6 to 12 July 2024. - Reference: https://admission.itb.ac.id/info/international-undergraduate-program/'),\n",
                            " Document(metadata={'title': 'ITB IUP Program Sched', 'sheet_name': 'ITB IUP Program Sched'}, page_content='Tuition fee and institution development donation payments for new students will be due in two separate periods. The first period will be from 6 to 10 May 2024, while the second period will be from 8 to 15 July 2024.\\n\\nThe new student commencement ceremony will be held at a later date, which will be announced separately. \\n\\nThe first day of lectures for the ITB International Undergraduate Program will take place on 1 August 2024. - Reference: https://admission.itb.ac.id/info/international-undergraduate-program/'),\n",
                            " Document(metadata={'title': 'IUP Fee Component', 'sheet_name': 'IUP Fee Component'}, page_content='The fees for the program can be broken down into several components. The first component is the registration and selection test, which costs IDR 4,000,000 for both the international class and international track, and also IDR 4,000,000 for foreign nationals, with an additional USD 255 for foreigners.\\n\\nThe tuition fee per semester is the second component, costing IDR 30,000,000 for both the international class and international track, and also IDR 30,000,000 for foreign nationals, with an additional USD 1900 for foreigners.\\n\\nAdditionally, there is a one-time institution development donation of IDR 25,000,000 for all classes, including international, international track, and foreign nationals, with an additional USD 2250 for foreigners.\\n\\nThe outbound exchange fee varies depending on the requirements of ITB partner universities, with the same cost applying to all classes, including international, international track, and foreign nationals. - Reference: https://admission.itb.ac.id/info/international-undergraduate-program/'),\n",
                            " Document(metadata={'title': 'IUP Fee Component', 'sheet_name': 'IUP Fee Component'}, page_content='Lastly, the tuition fee scholarship is not available for the international class, but is limited for foreign nationals. - Reference: https://admission.itb.ac.id/info/international-undergraduate-program/')]"
                        ]
                    },
                    "execution_count": 86,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "all_documents[-5:]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Embedding and Inserting to Vector Database\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_texts = [doc.page_content for doc in all_documents]\n",
                "\n",
                "# embeddings = [embedding_model.embed_query(text) for text in input_texts]\n",
                "embeddings = embedding_model.embed_documents(input_texts)\n",
                "\n",
                "# embeddings = []\n",
                "\n",
                "# for i, text in enumerate(input_texts):\n",
                "#     print(f\"Processing document {i+1}/{len(input_texts)}\")\n",
                "#     embeddings.append(embedding_model.embed_query(text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_20391/2665297733.py:9: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
                        "  client.recreate_collection(\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
                        ]
                    },
                    "execution_count": 88,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "client = QdrantClient(\n",
                "    os.getenv(\"VECTOR_DB_ENDPOINT\"),\n",
                "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
                "    prefer_grpc=True,\n",
                ")\n",
                "# client = QdrantClient(os.getenv(\"LOCAL_VECTOR_DB_ENDPOINT\"))\n",
                "\n",
                "\n",
                "client.recreate_collection(\n",
                "    collection_name=\"informasi_umum_json_refs\",\n",
                "    vectors_config=VectorParams(size=len(embeddings[0]), distance=Distance.COSINE),\n",
                ")\n",
                "\n",
                "client.upsert(\n",
                "    collection_name=\"informasi_umum_json_refs\",\n",
                "    points=[\n",
                "        PointStruct(\n",
                "            id=i,\n",
                "            vector=vector,\n",
                "            payload={\n",
                "                \"page_content\": all_documents[i].page_content,\n",
                "                \"metadata\": all_documents[i].metadata,\n",
                "            },\n",
                "        )\n",
                "        for i, vector in enumerate(embeddings)\n",
                "    ],\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
